## 0. 使用场景: 遇事不决就 hash

1. [list]: [空间上差不多都是 ziplist] 只能替换某一条, hash 是可以修改大对象都是 ziplist 的某个属性
   - stack/queue/array/bqueue
2. [**hash**] 存放复杂对象 + 配置文件列表
3. [string]分布式锁
4. [string]限流控制: 漏桶 / 令牌 / 计数 / 拥塞窗口
5. [string]幂等控制
6. [string]预减库存/点赞
7. [string]缓存用戶信息: 登錄, 这里主要泛型擦除[java]
8. [string-bitmap]布隆过滤器
9. [string-bitmap]签到: bitmap + string
10. [string] 日活
11. [HyperLogLog]UV 统计: HyperLogLog
12. [zset]排行榜: 多字段排序 + hash 做的记录那个分数[因为涉及负载计算+rpc], `k(storeId) + v(object{usrid + score})`
13. [zet]_抽奖: 社区 bug 抽奖 + set 可以做, 但是最终还是使用了 mysql 在内存中做的_
14. [set]好友 set: 共同關注 + 粉絲集合 + 可能感興趣
15. [string/hash]应对并发: 只保证原始数据的落库, 其他操作都在 redis 里, 之后定时任务同步会数据库 + 最终态的校验
16. LUA 脚本批量删除: LUA 脚本是原子性的, 所以每次连接只删除一点, 多次连接`获取连接与阻塞时长的平衡`

## 1. 排行榜的实现问题

1. 数据修改之后会存数据库操作
2. 但是会维护 redis 的排行榜: 避免每次都去查询数据库
3. redis command

   ```shell
   # 1. 设置玩家分数: O(log(N))
   # zadd 排行榜名称 分数 玩家标识
   zadd lb 89 user1
   zadd lb 95 user2
   zadd lb 95 user3

   # 2. 查看玩家分数: O(1)
   # zscore 排行榜名称 玩家标识
   zscore lb user2

   # 3. 按名次查看排行榜: O(log(N)+M)
   # zrevrange 排行榜名称 起始位置 结束位置 [withscores]
   zrevrange lb 0 -1 withscores

   # 4. 查看玩家的排名: O(log(N))
   # zrevrank 排行榜名称 玩家标识
   zrevrank lb user3 #0

   # 5. 增减玩家分数: O(log(N)), 没有的话默认为0
   # zincrby 排行榜名称 分数增量 玩家标识
   zincrby lb 6 user4

   # 6. zrem: O(log(N))
   # zrem 排行榜名称 玩家标识
   zrem lb user4

   # 7. 删除排行榜
   del lb
   ```

## 2. 签到系统

1. 需求

   - 显示用户某个月的签到次数和首次签到时间
   - 判断用户登陆状态: **一个大 key 就可以做完: 具体操作都是微操作** + 要配合用户的 ID 系统一起
   - ~~两亿用户最近 7 天的签到情况, 统计 7 天内连续签到的用户总数~~: 报表数据可以走数据库[redis 是为了 toc 抗并发的] + redis 做的话 `key=yyMMdd <userId> value`
   - 两亿签到: 数据量巨大使用 string 是不合理的{sds+redisobj 等都会占用空间} + bitmap 好且可以操作 bit

2. 常用的统计模式

   - 二值状态统计: bitmap
   - 聚合统计[交并差补]: set
   - 排序统计: list/zset
   - 基数统计: hyperloglog

3. redis bitmap: bitmap 是基於 string 類型的按位與操作 + 最大位 2^32=512M

   - `一個人一月一個 key: upms:member:1:202001 0 1` || ~~`key=yyMMdd <userId> value`~~
   - 一人一月: 31 天 4byte
   - 一月: 4byte \* 1 千萬用戶 / 1024 /1024 = 38M
   - 一年: 38M \* 12 = 456M

4. redis command

   ```shell
   # 第一天簽到
   aliyun:14>setbit upms:member:sign:1:202108 0 1
   "0"
   # 第二天簽到
   aliyun:14>setbit upms:member:sign:1:202108 1 1
   "0"
   # 第11天簽到
   aliyun:14>setbit upms:member:sign:1:202108 10 1
   "0"
   # 第12天簽到
   aliyun:14>setbit upms:member:sign:1:202108 11 1
   "0"
   # 第14天簽到
   aliyun:14>setbit upms:member:sign:1:202108 13 1
   "0"
   # 第15天簽到
   aliyun:14>setbit upms:member:sign:1:202108 14 1
   "0"

   # 查看第15天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 14
   "1"
   # 查看第11天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 10
   "1"
   # 查看第13天簽到情況
   aliyun:14>getbit upms:member:sign:1:202108 12
   "0"
   # 查看202108月簽到次數
   aliyun:14>bitcount upms:member:sign:1:202108
   "6"

   # 查看202108月第一次簽到是第幾天
   aliyun:14>bitpos upms:member:sign:1:202108 1
   "0"
   # 查看202108月第一次沒簽到是第幾天
   aliyun:14>bitpos upms:member:sign:1:202108 0
   "2"

   # 查看202108月1-4號的簽到情況: 最終得到的是這些位上的數字轉long: u表示按照無符號轉換
   # 从 0 号位置开始取3位
   aliyun:14>bitfield upms:member:sign:1:202108 get u3 0
   1) "6"     110          表示第一天登录. 表示第二天登录, 表示第三天没登录.
   # aliyun:14>setbit upms:member:sign:1:202108 3 1
   # aliyun:14>bitfield upms:member:sign:1:202108 get u4 0
   # 1) "13"     1101  表示第一天登录. 表示第二天登录, 表示第三天没登录. 表示第四天登录
   aliyun:14>bitfield upms:member:sign:1:202108 get u4 0
   1) "12"
   # 查看202108月1-4號的簽到情況: 最終得到的是這些位上的數字轉long: i示按照有符號轉換
   aliyun:14>bitfield upms:member:sign:1:202108 get i3 0
   1) "-2"
   aliyun:14>bitfield upms:member:sign:1:202108 get i4 0
   1) "-4"
   ```

## 3. 日活: set/hash/bitmap/hyperloglog

1. set 但是过于占用空间: `sadd + scard `
2. hash: hash 的 key 也有去重功能 + `hset+hlen`
3. bitmap[本质是 string]: 最大的用户 Id 是 512M, 2^32=42 亿 + `GETBIT/SETBIT/bitcount`
4. 如果数据 Id 不是连续的获取数据量不大就不合适
5. redis command

   ```shell
   # key=XxyyMMdd + offset(uid)
   # 用户登录
   setbit login1111 uid-1 1
   setbit login1111 uid-2 1
   setbit login1111 uid-3 1
   setbit login1111 uid-4 1

   # strlen login1111  可以查看空间的占用
   bitcount login1111 0 1 # 注意这里的 0-1 指的是存储空间[len]
   # 查看日活
   bitcount login1111

   # 12 号的用户登录信息
   setbit login1112 uid-1 1
   setbit login1112 uid-4 1
   setbit login1112 uid-5 1

   # 连续两天都登录的人
   bitop and login1111-and-login1112  login1111 login1112
   # 两天一共登录的人
   bitop or login1111-or-login1112  login1111 login1112
   ```

## 4.分布式锁

### 本地缓存锁

![avatar](/static/image/db/redis-standalone.png)

1. 单机版在多线程下需要加锁
2. 确保以下 3 个动作是原子的, 二期是在锁住的方法内
   - query from redis cache
   - query database
   - put result to redis cache

### 分布式缓存锁

![avatar](/static/image/dist/dist-lock.png)

1. 集群微服务: 单机版锁值管自己的 JVM 层面的, 可能会导致 2 个服务买同一个商品
2. core: 在指定地方占一个位置, 占上位置就可以执行逻辑, 否则等待
   ![avatar](/static/image/db/redis-distribute.png)
3. 分布式锁的注意点

   - 加锁的原子性: `SET K V NX EX`
   - 解**自己**锁的原子性: lua 或者 ~~redis 事务: watch + transaction+ multi+delete + unwatch~~
     ```lua
     // 返回值是 Long
     if redis.call('get',KEYS[1]) == ARGV[1] then
       return redis.call('del',KEYS[1])
     else return 0
     end
     ```
   - 解/删除锁代码的一定执行 + ex
   - 解决业务超时问题锁续期问题 + 只能删除自己的锁
   - redis 集群时异同同步数据导致的 set 丢失问题: 自己手动修复数据

4. evolution

   ![avatar](/static/image/db/redis-distribute-v1.png)
   ![avatar](/static/image/db/redis-distribute-v2.png)
   ![avatar](/static/image/db/redis-distribute-v3.png)
   ![avatar](/static/image/db/redis-distribute-v4.png)

5. redission 也还是会出现主从同步锁丢失的问题:

   - Zookeeper 可以解决这个问题(CP)
   - 也可以使用 RedLock 向多个 redis 内加锁, 半数成功才认为加锁成功

6. java 代码中 STW 会有续期的问题

   - zk 临时节点在 stw 时没有心跳
   - redis 也是的

7. 继续优化

   - 分段锁: 将库存分为多个阶段, 对每个阶段进行加锁减库存操作
   - 读写锁

8. RedLock: 一般没人会用

   - 为了解决主从架构下的锁丢失问题: 发生的情况就很少
   - 官方建议在不同机器上部署 5 个 Redis 主节点: 5-单数个[节点容错], 节点独立对等, 没有主从
   - flow: 客户端要获取锁有 5 个步骤

     1. 客户端获取当前时间 T1[毫秒级别]
     2. 使用相同的 key 和 value 顺序尝试从 N 个 Redis 实例上获取锁: 每个请求都有毫秒级别的超时时间[快速向下一个实例发送请求]
     3. 客户端获取当前时间 T2 并减去 T1 来计算出获取锁所用的时间 T3 = T2 -T1:
        - 当且仅当客户端在大多数实例[N/2 + 1]获取成功, 且获取锁所用的总时间 T3 小于锁的有效时间, 才认为加锁成功, 否则加锁失败
     4. 如果第 3 步加锁成功, 则执行业务逻辑操作共享资源: key 的真正有效时间等于有效时间减去获取锁所使用的时间(T3)
     5. 如果获取锁失败[没有 N/2+1 个 server/取锁时间已经超过了有效时间], 客户端应该在所有的 Redis 实例上进行解锁[即便某些 Redis 实例根本就没有加锁成功]

   - 缺点: Martin 分布式大佬

     1. 锁定的目的是为了保护对共享资源的读写, 分布式锁应该「高效」和「正确」: 5 台 server 复杂且不高效 + 只有一个线程能对共享数据读写
     2. `5 台 server 很重 === redlock 作者说可以解决网络延迟、进程暂停`
     3. `默认自带假设: 多个节点机器时钟都是一致 === redlock 作者说大概一致就行[现在的 server 基本都能做到]`
     4. 无法保证正确性
     5. 请使用 Zookeeper

## 5. 分布式缓存: 热点高频尽量不变的对象

1. 客户端缓存: 页面/浏览器缓存, APP 缓存, H5 缓存, Localstrage, session-storage
2. CDN 缓存: 内容存储, 数据存储, 内容分发[负载均衡]
3. nginx 缓存: 静态资源
4. 服务器缓存: 本地缓存, 外部缓存
5. 数据库缓存: 持久层缓存[mybatis 缓存], mysql 服务的缓存
6. 操作系统缓存: Page Cahce, Buffer Cache

### 缓存数据的一致性问题

![avatar](/static/image/db/redis-consistency-data.png)

1. 从设计思路来保证**数据一致性**{三种}

   - [旁路缓存模式]Cache Aside: **读请求比较多** + 把缓存责任交给应用层{从性能极致来说} + 同时维系 DB 和 cache{并且是以 DB 的结果为准}
   - [读写穿透]Read/Write Through: 服务端把 cache 视为主要数据存储, 从中读取数据并将数据写入其中, cache 服务负责将此数据读取和写入 DB, 从而减轻了应用程序的职责{很少使用(redis 不支持写 db)}

     1. 写入: 先查 cache, cache 中不存在, 直接更新 DB; cache 中存在, 则先更新 cache, 然后 cache 服务自己更新 DB[**同步更新** cache 和 DB]
     2. 读取: 从 cache 中读取数据, 读取到就直接返回; 读取不到的话, 先从 DB 加载, 写入到 cache 后返回响应
     3. cache(相当于 proxy) 服务自己来写入缓存的: 客户端是透明的

   - [异步缓存写入]Write Behind Pattern: **由 cache 服务来负责 cache 和 DB 的读写**(与读写穿透类似)

     1. Read/Write Through 是同步更新 cache 和 DB
     2. Write Behind Caching 则是只更新缓存, 不直接更新 DB, 而是改为异步批量的方式来更新 DB
     3. 写性能非常高: 适合一些数据经常变化又对数据一致性要求没那么高的场景(如浏览量、点赞量)
     4. 应用: **MySQL 的 InnoDB Buffer Pool 机制** || 消息队列中消息的异步写入磁盘

2. Cache Aside 解决方案

   - [失效模式]数据存到数据库中成功后, 再让缓存失效{lazy 计算的思想}: 等到读缓存不命中的时候再加载进去
   - [双写模式]先更新数据, 再更新缓存: 把 Cache 也做 Buffer 用
   - 通过消息队列更新缓存
   - [canal]起一个同步服务, 作为 MySQL 一个从节点, 通过解析 binlog 同步重要缓存{超高并发或一致性要求高不适合 + **最终一致性的最优解**}

   ![avatar](/static/image/db/redis-consistency-update.png)

3. 双写模式: `第二个线程写缓存先执行了导致`的不一致, **而且缓存有时不是单纯的数据[经过相关的逻辑结果、频繁修改但是没有使用(更新缓存是没有意义的)]**

   ![avatar](/static/image/db/redis-data-consistency.png)

   - 可以串行化: 但是效率太低

4. **失效(可以使用 queue)模式**: 先写数据库, 之后再删除缓存{失败[自己封装的服务]-retry}

   - `第三个执行查询写入 redis 快于第二个, 会导致多查数据库`
   - `1 删除结束, 3 进行数据库查询, 2 更新了数据库并删除了缓存, 3 开始写如redis, 此时的数据就不会死最新的[2操作后的数据时最新的]`
   - **不一致概率非常小, 因为缓存的写入速度是比数据库的写入速度快很多**
   - 超高并发下不: **重试删除缓存就完全可以做成异步**
   - 存在的问题:

     1. 首次请求数据一定不存在 cache 的问题
     2. 写操作比较频繁的话导致 cache 中的数据会被频繁被删除, 会影响缓存命中率(鸵鸟 || 双写)

     ![avatar](/static/image/db/redis-data-consistency-v2.png)

   - solution1: 可以使用读写锁, 2 在写时, 3 读会被阻塞
   - solution2: 使用 BlockingQueue 进行排序{更新数据的时候, 根据**数据的唯一标识**, 将操作路由之后, 发送到一个 jvm 内部队列中}
     1. 数据变更的操作: 先删除缓存, 然后再去更新数据库, 但是还没完成更新
     2. 此时读请求过来: 读到了空的缓存, 先将缓存更新的请求发送到队列中, 此时会在队列中积压, 然后同步等待缓存更新完成
     3. 一个队列中, 其实多个连续的更新缓存请求串在一起是没意义的: 可以过滤
     4. 缺点： 复杂 + 读请求长时阻塞 + 唯一数据的倾斜

5. 失效模式: 先删除缓存, 之后再写数据库

   - issue: 删除缓存失败则数据就不对了 & 删除缓存之后写入数据库之前有查询则缓存旧的数据
   - solution3: 延时双删[先删除缓存, 在写数据库, 之后休眠一会才删除缓存]

6. **延时双删**: 数据一致性有点点差

   - {线程池}异步~~串行化~~删除: ~~删除请求入队列~~
   - 休眠时间: 在读数据业务逻辑的耗时的基础上, 加上几百 ms 即可
   - 缓存删除失败: 重试机制, 可以借助消息队列的重试机制, 也可以自己整个表, 记录重试次数

7. 解决方案

   - 如果时用户维度的数据(订单数据, 用户数据), 并发几率小, `缓存+过期时间+失效模式` 就可以了
   - 如果时菜单, 商品介绍等数据[可以长时间数据不一致]: `canal-binlog`
   - 要求一致性高的可以考虑加锁: `读写锁`
   - 遇到实时性一致性要求超高的, 应该直接查询数据库

### 使用场景: `缓存只保证最终一致性`

0. 即使是这种单机应用, IO 文件也不是每次都 fsync 存盘的 fsync 会带来性能的急剧下降
1. 即时性, 数据一致性要求不高: _物流信息_, _商品分类_
   - 缓存本来就有时差性(应该的), 如果是追求严格意义上的一致性则不要使用缓存
2. 访问量大, 但是更新频率不高的数据`[读多写少]`: _商品信息_
3. 但是还是要尽量减少 DB 的操作: 删除缓存其实也不是太好的方式
4. **遇到实时性一致性要求超高的, 应该直接查询数据库**
5. 好的方案是使用 canal 或者使用封装好的服务[中台]

### 缓存分类

1. 本地缓存

   - 比如存在 Map 中, 只适用于单体应用
   - 分布式下存在数据不一致问题

2. 分布式缓存

## 6. 分布式限流[拥塞窗口/令牌桶]

1. 在微服务架构下, 限频器也需要分布式化: 无论是哪种算法, 都可以结合 Redis 来实现
2. Redis 负责管理令牌, 微服务需要进行函数操作, 就向 Redis 申请令牌, 如果 Redis 当前还有令牌, 就发放给它. 拿到令牌, 才能进行下一步操作
3. 另一方面, 令牌不光要消耗, 还需要补充, 出于性能考虑, 可以使用懒生成的方式
   - 使用令牌时, 顺便生成令牌
   - 好处: 令牌的获取, 和令牌的生成, 都可以在一个 Lua 脚本中, 保证了原子性
